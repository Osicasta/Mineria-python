# -*- coding: utf-8 -*-
"""Clasificacion_CalidadAgua.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CVHAlutrhmrTjz2mQupKR41dWiP0Cl-d

# Clasificación de Calidad del agua

1. Preparación de Datos
2. División de los datos 70-30
3. Aprendizaje del Modelo: Tree, Knn, NN, SVM, RandomForest
4. Evaluación del Modelo: matriz de confusion, P,R, ROC
5. Guardar el modelo

* El despliegue se realiza en otro jupyter_notebook

# 1. Preparación de datos
"""

#Importamos librerías básicas
import pandas as pd # manipulacion dataframes
import numpy as np  # matrices y vectores
import matplotlib.pyplot as plt #gráfica

#Cargamos los datos
data = pd.read_csv("datos_preparados_calidad_agua.csv")
data.head()

#Conocimiento de los datos
data.info()

#Corrección del tipo de datos object a categorías
data['Departamento']=data['Departamento'].astype('category')
data['MunicipioCodigo']=data['MunicipioCodigo'].astype('category')
data['Municipio']=data['Municipio'].astype('category')
data['Año']=data['Año'].astype('category')
data['Nivel de riesgo']=data['Nivel de riesgo'].astype('category')
data['Nivel de riesgo urbano']=data['Nivel de riesgo urbano'].astype('category')

data.info()

#Eliminacion de una variable irrelevante
data = data.drop('Unnamed: 0',axis=1)
data.info()

#Ya se realizo la preparacion de los datos anteriormente
#Se crean dummies a las variables predictoras categóricas (no a la variable obj)
data = pd.get_dummies(data, columns=['Departamento','MunicipioCodigo','Municipio','Año','Nivel de riesgo urbano'], drop_first=False, dtype=int)
data.head()

#Se codifican las categorias de la VARIABLE OBJETIVO
#En clasificacion siempre se aplica labelEncoder a la variable objetivo

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
data["Nivel de riesgo"]=labelencoder.fit_transform(data["Nivel de riesgo"]) #Objetivo

data.head()

"""# 2. Division 70 - 30"""

from sklearn.model_selection import train_test_split
X = data.drop("Nivel de riesgo", axis = 1) # Variables predictoras
Y = data['Nivel de riesgo'] #Variable objetivo
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y) #Siempre el muestreo estratificado en clasificacion
Y_train.value_counts().plot(kind='bar')# Objetivo del 70%

"""# 3. Aprendizaje con el 70% y evaluacion con el 30%

# **Tree**
Transformacion y discretizacion. NO se normaliza
"""

#Creación del modelo con el conjunto de entrenamiento
from sklearn.tree import DecisionTreeClassifier

modelTree = DecisionTreeClassifier(criterion='gini', min_samples_leaf=2, max_depth=None) #gini, entropy
modelTree.fit(X_train, Y_train) #70% train

#Grafica del arbol
from sklearn.tree import plot_tree
plt.figure(figsize=(6,8))
plot_tree(modelTree, feature_names=X_train.columns.values, class_names=labelencoder.classes_, rounded=True, filled=True)
plt.show()

"""**Evaluacion**"""

#Evaluación 30%
from sklearn import metrics

Y_pred = modelTree.predict(X_test) #30% Test
print(Y_pred)

#Exactitud
exactitud=metrics.accuracy_score(y_true=Y_test, y_pred=Y_pred)
print(exactitud)

#Matriz de confusion
from sklearn import metrics

cm=metrics.confusion_matrix(y_true=Y_test, y_pred=Y_pred)
cm

#Plot de la matriz de confusion
disp=metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labelencoder.classes_)
disp.plot()

#Precision, Recall, f1, exactitud
print(metrics.classification_report( y_true=Y_test, y_pred=Y_pred, target_names=labelencoder.classes_))

import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.preprocessing import label_binarize
import numpy as np


# Binarizar las etiquetas de prueba
Y_test_bin = label_binarize(Y_test, classes=modelTree.classes_)
n_classes = Y_test_bin.shape[1]

# Obtener las probabilidades de predicción para cada clase
y_score = modelTree.predict_proba(X_test)

# Calcular la curva ROC y el AUC para cada clase
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = metrics.roc_curve(Y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = metrics.auc(fpr[i], tpr[i])

# Graficar las curvas ROC para cada clase
plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Curva ROC de la clase {modelTree.classes_[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Umbral Aleatorio')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curvas ROC One-vs-Rest para Clasificación Multiclase')
plt.legend(loc="lower right")
plt.show()

"""**Conclusión**

El modelo de árbol de decisión (Tree) demuestra un desempeño excepcional en términos de calidad predictiva. Los resultados obtenidos reflejan una exactitud perfecta del 100%, confirmada tanto por la métrica de precisión global como por la matriz de confusión, en la que todos los valores están correctamente clasificados, sin errores de predicción entre las clases.

Asimismo, el informe de clasificación indica valores perfectos (1.00) en precisión, recall, y f1-score para cada una de las clases: Inviable sanitariamente, Riesgo alto, Riesgo bajo, Riesgo medio y Sin riesgo. Esto sugiere que el modelo no solo acierta en la mayoría de los casos, sino que también mantiene un equilibrio ideal entre los falsos positivos y negativos.
Complementariamente, las curvas ROC para cada clase muestran un área bajo la curva (AUC) de 1.00, lo que implica una capacidad de discriminación total entre las clases del problema, reforzando la idea de un modelo perfectamente entrenado.

No obstante, resultados tan perfectos pueden ser indicio de un posible sobreajuste (overfitting), especialmente si el comportamiento del modelo en los datos de entrenamiento y prueba es idéntico.

# **Random Forest**
NO se debe normalizar
"""

#Random Forest
from sklearn.ensemble import RandomForestClassifier

model_rf= RandomForestClassifier(n_estimators=100,  max_samples=0.7, criterion='gini',
                              max_depth=None, min_samples_leaf=2)
model_rf.fit(X_train, Y_train) #70%

#No se realiza grafica

"""**Evaluacion**"""

#Evaluación de RandomForest con 30%
from sklearn import metrics

Y_pred = model_rf.predict(X_test) #30%

#Matriz de confusion
cm=metrics.confusion_matrix(y_true=Y_test, y_pred=Y_pred)
disp=metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labelencoder.classes_)
disp.plot()

#Precision, Recall, f1, exactitud
print(metrics.classification_report( y_true=Y_test, y_pred=Y_pred, target_names=labelencoder.classes_))

import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.preprocessing import label_binarize
import numpy as np


# Binarizar las etiquetas de prueba
Y_test_bin = label_binarize(Y_test, classes=model_rf.classes_)
n_classes = Y_test_bin.shape[1]

# Obtener las probabilidades de predicción para cada clase
y_score = model_rf.predict_proba(X_test)

# Calcular la curva ROC y el AUC para cada clase
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = metrics.roc_curve(Y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = metrics.auc(fpr[i], tpr[i])

# Graficar las curvas ROC para cada clase
plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Curva ROC de la clase {model_rf.classes_[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Umbral Aleatorio')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curvas ROC One-vs-Rest para Clasificación Multiclase')
plt.legend(loc="lower right")
plt.show()

"""**Conclusión**

El modelo Random Forest muestra un desempeño sólido y confiable en la clasificación multiclase, evidenciado por una exactitud general del 97 %. La matriz de confusión muestra una distribución de errores muy baja, con la mayoría de las instancias correctamente clasificadas, especialmente en las clases “Inviable sanitariamente” y “Sin riesgo”, que presentan una alta precisión y recall (0.98 y 0.99 respectivamente).

A nivel general, se destacan los siguientes indicadores:
- Precision macro promedio: 0.97
- Recall macro promedio: 0.97
- F1-score macro promedio: 0.97

Estos valores indican que el modelo mantiene un buen equilibrio entre precisión y sensibilidad para todas las clases, incluso en presencia de posibles desbalances.

Adicionalmente, las curvas ROC por clase revelan un AUC promedio elevado, donde la clase 0 (Inviable sanitariamente) alcanza un AUC perfecto de 1.00, y las demás clases superan el 0.85, lo que refuerza la capacidad del modelo para distinguir correctamente entre clases.

# **KNN**
Se debe de normalizar
"""

#Normalizacion las variables numéricas (las dummies no se normalizan)
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler()
min_max_scaler.fit(data[['IRCA','IRCAurbano']]) #Ajuste de los parametros: max - min

#Se aplica la normalización a 70%  y 30%
X_train[['IRCA','IRCAurbano']]= min_max_scaler.transform(X_train[['IRCA','IRCAurbano']]) #70%
X_test[['IRCA','IRCAurbano']]= min_max_scaler.transform(X_test[['IRCA','IRCAurbano']]) #30%
X_train.head()

"""**Aprendizaje**"""

#Aprendizaje KNN con 70%
from sklearn.neighbors  import KNeighborsClassifier #KNeighborsRegressor

modelKnn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')#euclidean, minkowski
modelKnn.fit(X_train, Y_train) #70%

"""**Evaluacion**"""

#Evaluación de Knn con 30%
from sklearn import metrics

Y_pred = modelKnn.predict(X_test) #30%

#Matriz de confusion
cm=metrics.confusion_matrix(y_true=Y_test, y_pred=Y_pred)
disp=metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labelencoder.classes_)
disp.plot()

#Precision, Recall, f1, exactitud
print(metrics.classification_report( y_true=Y_test, y_pred=Y_pred, target_names=labelencoder.classes_))

import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.preprocessing import label_binarize
import numpy as np


# Binarizar las etiquetas de prueba
Y_test_bin = label_binarize(Y_test, classes=modelKnn.classes_)
n_classes = Y_test_bin.shape[1]

# Obtener las probabilidades de predicción para cada clase
y_score = modelKnn.predict_proba(X_test)

# Calcular la curva ROC y el AUC para cada clase
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = metrics.roc_curve(Y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = metrics.auc(fpr[i], tpr[i])

# Graficar las curvas ROC para cada clase
plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Curva ROC de la clase {modelKnn.classes_[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Umbral Aleatorio')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curvas ROC One-vs-Rest para Clasificación Multiclase')
plt.legend(loc="lower right")
plt.show()

"""**Conclusión**

El modelo KNN muestra un desempeño aceptable en la clasificación multiclase, alcanzando una precisión global del 92%. La matriz de confusión revela que el modelo tiende a confundir clases con características similares, especialmente entre categorías de riesgo. Las métricas individuales por clase reflejan cierta variabilidad en el rendimiento, destacándose las clases “inviable sanitariamente” y “sin riesgo” con valores F1 altos, mientras que “riesgo medio” y “riesgo bajo” presentan mayor dificultad de predicción.
Las curvas ROC indican una capacidad discriminativa sólida, con valores AUC entre 0.92 y 1.00, lo cual respalda la buena sensibilidad del modelo en la mayoría de las clases. En conjunto, KNN es un modelo funcional, aunque muestra limitaciones cuando las clases presentan fronteras difusas o solapamiento.

# **Red neuronal**
Se debe de normalizar

**Aprendizaje**
"""

#Red Neuronal

from sklearn.neural_network import MLPClassifier #MLPRegressor

#Solo se configura capas ocultas, no se configura capa de entrada y de salida
modelNN = MLPClassifier(activation="logistic",hidden_layer_sizes=(5), learning_rate='constant',
                     learning_rate_init=0.2, momentum= 0.3, max_iter=500, random_state=3)

modelNN.fit(X_train, Y_train) #70% normalizados

#Loss es la desviación entre Y_train y el Y_pred
loss_values = modelNN.loss_curve_
plt.plot(loss_values)

"""**Evaluacion**"""

#Evaluación de Red Neuronal
from sklearn import metrics

Y_pred = modelNN.predict(X_test) #30%

#Matriz de confusion
cm=metrics.confusion_matrix(y_true=Y_test, y_pred=Y_pred)
disp=metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labelencoder.classes_)
disp.plot()

#Precision, Recall, f1, exactitud
print(metrics.classification_report( y_true=Y_test, y_pred=Y_pred, target_names=labelencoder.classes_))

import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.preprocessing import label_binarize
import numpy as np


# Binarizar las etiquetas de prueba
Y_test_bin = label_binarize(Y_test, classes=modelNN.classes_)
n_classes = Y_test_bin.shape[1]

# Obtener las probabilidades de predicción para cada clase
y_score = modelNN.predict_proba(X_test)

# Calcular la curva ROC y el AUC para cada clase
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = metrics.roc_curve(Y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = metrics.auc(fpr[i], tpr[i])

# Graficar las curvas ROC para cada clase
plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Curva ROC de la clase {modelNN.classes_[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Umbral Aleatorio')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curvas ROC One-vs-Rest para Clasificación Multiclase')
plt.legend(loc="lower right")
plt.show()

"""**Conclusión**

Este modelo utilizó una arquitectura multicapa, adecuada para clasificaciones multiclase, y fue entrenado con un conjunto de datos balanceado que abarca cinco categorías de riesgo.
Durante la fase de evaluación, se aplicó una matriz de confusión que evidenció una correcta identificación de la mayoría de las instancias. La red presentó métricas destacadas: precisión, recall y f1-score superiores al 94 % en todas las clases, lo cual denota una capacidad equilibrada para identificar correctamente tanto los positivos como los negativos. La exactitud general fue de 94 %, mientras que el AUC de las curvas ROC por clase osciló entre 0.98 y 1.00, lo que indica una excelente capacidad de discriminación del modelo en escenarios multiclase.

# **SVM**

**Aprendizaje**
"""

from sklearn.svm import SVC # SVR -> Regresion #SVC -> Clasificacion

modelSVM = SVC(kernel='linear',probability=True) #'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'
modelSVM.fit(X_train, Y_train) #70%

"""**Evaluacion**"""

#Evaluación de SVM
from sklearn import metrics

Y_pred = modelSVM.predict(X_test) #30%

#Matriz de confusion
cm=metrics.confusion_matrix(y_true=Y_test, y_pred=Y_pred)
disp=metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labelencoder.classes_)
disp.plot()

#Precision, Recall, f1, exactitud
print(metrics.classification_report( y_true=Y_test, y_pred=Y_pred, target_names=labelencoder.classes_))

import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.preprocessing import label_binarize
import numpy as np


# Binarizar las etiquetas de prueba
Y_test_bin = label_binarize(Y_test, classes=modelSVM.classes_)
n_classes = Y_test_bin.shape[1]

# Obtener las probabilidades de predicción para cada clase
y_score = modelSVM.predict_proba(X_test)

# Calcular la curva ROC y el AUC para cada clase
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = metrics.roc_curve(Y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = metrics.auc(fpr[i], tpr[i])

# Graficar las curvas ROC para cada clase
plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Curva ROC de la clase {modelSVM.classes_[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Umbral Aleatorio')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curvas ROC One-vs-Rest para Clasificación Multiclase')
plt.legend(loc="lower right")
plt.show()

"""**Conclusión**

La matriz de confusión evidencia una clasificación muy precisa en todas las clases, con errores mínimos. Las métricas de desempeño muestran una precisión, recall y f1-score superiores al 95 % para todas las clases, alcanzando una exactitud global de 96 %. Esto refleja que el modelo no solo es capaz de identificar correctamente cada clase, sino que también lo hace de forma muy equilibrada y consistente.
La calidad de clasificación alcanzada por el modelo SVM indica una excelente generalización, con bajo sobreajuste y una alta capacidad para capturar patrones complejos en los datos.

# **5. Guardar el modelo**
"""

import pickle
filename = 'modelo_analitica.pkl'
variables= X.columns._values
pickle.dump([modelTree,model_rf,modelKnn, modelNN, modelSVM, labelencoder,variables,min_max_scaler], open(filename, 'wb'))

"""# **Hiperparametrizacion con el mejor modelo**

El mejor modelo según la evaluación del mismo es el ModelTree
"""

#  Arbol
from sklearn.tree import DecisionTreeClassifier
modelTree = DecisionTreeClassifier()

# Definir los hiperparametros
criterion=['entropy','gini'] #Indice de información
min_samples_leaf=[2,10,50,100] # Cantidad de registros por hoja
max_depth=[None, 10,20,50] #Niveles de profundidad

#Hiperparametrización
from sklearn.model_selection import GridSearchCV

param_grid = dict(criterion=criterion, min_samples_leaf=min_samples_leaf, max_depth=max_depth) #Se escribe para cada metodo por separado dependiendo de los parametros
grid = GridSearchCV(estimator=modelTree, param_grid=param_grid, scoring='f1_macro', n_jobs=-1, cv=10) #Deben de ser cv=10 #Para clasificacion: f1_macro # Para regresion: MSE, MAPE, MAE
grid.fit(X_train, Y_train) #70%

#Mejor modelo
modelTree= grid.best_estimator_

#Medida de evaluación del mejor modelo
medidas= pd.DataFrame(index=['f1 de la CV'])
medidas['Tree']=grid.best_score_

# Mejores párametros
print( grid.best_params_)
print(medidas)

#Mejor modelo
from sklearn.tree import plot_tree
plt.figure(figsize=(5,5))
plot_tree(modelTree, feature_names=X_train.columns.values, class_names=labelencoder.classes_, rounded=True, filled=True, fontsize=8)
plt.show()

"""# **Evaluacion del mejor modelo con el 30%**

**Arbol**
"""

#Evaluación del mejor Tree con el 30% (X_test sin normalizar)
from sklearn import metrics

Y_pred = modelTree.predict(X_test) #30%

#Matriz de confusion
cm=metrics.confusion_matrix(y_true=Y_test, y_pred=Y_pred)
disp=metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labelencoder.classes_)
disp.plot()

#Precision, Recall, f1, exactitud
print(metrics.classification_report( y_true=Y_test, y_pred=Y_pred, target_names=labelencoder.classes_))

# Curva ROC
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.preprocessing import label_binarize
import numpy as np


# Binarizar las etiquetas de prueba
Y_test_bin = label_binarize(Y_test, classes=modelTree.classes_)
n_classes = Y_test_bin.shape[1]

# Obtener las probabilidades de predicción para cada clase
y_score = modelTree.predict_proba(X_test)

# Calcular la curva ROC y el AUC para cada clase
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = metrics.roc_curve(Y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = metrics.auc(fpr[i], tpr[i])

# Graficar las curvas ROC para cada clase
plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Curva ROC de la clase {modelTree.classes_[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Umbral Aleatorio')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curvas ROC One-vs-Rest para Clasificación Multiclase')
plt.legend(loc="lower right")
plt.show()

"""# **Guardar el modelo**"""

import pickle
filename = 'modelo-clas-hiper.pkl'
variables= X.columns._values
pickle.dump([modelTree,labelencoder,variables,min_max_scaler], open(filename, 'wb')) #write